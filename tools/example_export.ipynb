{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2f4d7a",
   "metadata": {},
   "source": [
    "# PyTorch模型ONNX导出和推理示例 （源于 Pytorch 官方教程）\n",
    "\n",
    "本notebook展示了如何将PyTorch模型导出为ONNX格式，并使用ONNX Runtime进行推理。主要包含以下步骤：\n",
    "\n",
    "1. 加载和初始化PyTorch模型\n",
    "2. 导出模型为ONNX格式\n",
    "3. 验证ONNX模型\n",
    "4. 使用ONNX Runtime进行推理\n",
    "5. 性能对比测试\n",
    "6. 实际图像处理示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450428c",
   "metadata": {},
   "source": [
    "## 1. 模型初始化\n",
    "\n",
    "首先导入必要的库并初始化测试模型。这里使用的是自定义的test_network模型，输入通道数为3，输出类别数为10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7c195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_network(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "from models.test_model2 import test_network\n",
    "\n",
    "# 设置批次大小\n",
    "batch_size = 16\n",
    "\n",
    "# 创建测试网络实例：3个输入通道，10个输出类别\n",
    "model = test_network(3, 10)\n",
    "# 设置为评估模式，关闭dropout和batch normalization\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ca6aa",
   "metadata": {},
   "source": [
    "## 2. 导出ONNX模型\n",
    "\n",
    "将PyTorch模型导出为ONNX格式。ONNX（Open Neural Network Exchange）是一个开放的深度学习模型交换格式，支持跨平台和跨框架的模型部署。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "017b0bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2986956/2290931629.py:7: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "# 创建模拟输入数据：批次大小16，3通道，224x224分辨率\n",
    "x = torch.randn(16, 3, 224, 224, requires_grad=True)\n",
    "# 使用PyTorch模型进行前向推理，获取输出作为参考\n",
    "torch_out = model(x)\n",
    "\n",
    "# 导出PyTorch模型为ONNX格式\n",
    "torch.onnx.export(\n",
    "    model,  # 要导出的PyTorch模型\n",
    "    x,  # 模型输入（用于追踪计算图）\n",
    "    \"../temp_data/test.onnx\",  # 保存路径\n",
    "    export_params=True,  # 是否导出模型参数权重\n",
    "    do_constant_folding=True,  # 是否执行常量折叠优化\n",
    "    input_names=[\"input\"],  # 输入节点名称\n",
    "    output_names=[\"output\"],  # 输出节点名称\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},  # 动态轴：支持可变批次大小\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f6ec17",
   "metadata": {},
   "source": [
    "## 3. 验证ONNX模型\n",
    "\n",
    "加载导出的ONNX模型并验证其结构是否正确。这是确保模型导出成功的重要步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da0ec9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入ONNX库\n",
    "import onnx\n",
    "\n",
    "# 加载导出的ONNX模型\n",
    "onnx_model = onnx.load(\"super_resolution.onnx\")\n",
    "# 检查模型结构是否有效\n",
    "onnx.checker.check_model(onnx_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d5e0f",
   "metadata": {},
   "source": [
    "## 4. ONNX Runtime推理测试\n",
    "\n",
    "使用ONNX Runtime加载模型并进行推理，然后与原始PyTorch模型的输出进行对比，确保转换的准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5a9cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX模型已通过ONNXRuntime测试，结果与PyTorch模型一致！\n"
     ]
    }
   ],
   "source": [
    "# 导入ONNX Runtime和NumPy\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# 创建ONNX Runtime推理会话，使用CPU执行提供程序\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"super_resolution.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    \"\"\"将PyTorch张量转换为NumPy数组的辅助函数\"\"\"\n",
    "    return (\n",
    "        tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "# 使用ONNX Runtime进行推理\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# 比较ONNX Runtime和PyTorch的输出结果\n",
    "# 使用较小的容差来验证数值精度\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"ONNX模型已通过ONNXRuntime测试，结果与PyTorch模型一致！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98fa9b5",
   "metadata": {},
   "source": [
    "## 5. 性能对比测试\n",
    "\n",
    "比较PyTorch模型和ONNX模型的推理速度，通常ONNX Runtime在某些场景下可以提供更好的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3758997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch模型推理耗时: 0.039594 秒\n",
      "ONNX模型推理耗时: 0.011369 秒\n"
     ]
    }
   ],
   "source": [
    "# 导入时间模块用于性能测试\n",
    "import time\n",
    "\n",
    "# 创建新的测试输入数据\n",
    "x = torch.randn(batch_size, 3, 224, 224, requires_grad=True)\n",
    "\n",
    "# 测试PyTorch模型推理时间\n",
    "start = time.time()\n",
    "torch_out = model(x)\n",
    "end = time.time()\n",
    "print(f\"PyTorch模型推理耗时: {end - start:.6f} 秒\")\n",
    "\n",
    "# 测试ONNX模型推理时间\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "start = time.time()\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "end = time.time()\n",
    "print(f\"ONNX模型推理耗时: {end - start:.6f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56efbc5b",
   "metadata": {},
   "source": [
    "## 6. 实际图像处理示例\n",
    "\n",
    "使用导出的ONNX模型处理实际图像。这个示例展示了如何使用加载硬盘中的图像并输出对应的概率。\n",
    "\n",
    "**处理流程：**\n",
    "1. 加载图像并调整大小\n",
    "2. 将硬盘中的数据输入到网络中\n",
    "3. 输出网络结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2f8d383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出图像形状: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# 导入图像处理库\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 加载测试图像\n",
    "img = Image.open(\"../temp_data/cat.png\")\n",
    "\n",
    "# 调整图像大小到224x224（与模型输入尺寸匹配）\n",
    "resize = transforms.Resize([224, 224])\n",
    "img = resize(img)\n",
    "\n",
    "# 将图像转换为张量格式\n",
    "to_tensor = transforms.ToTensor()\n",
    "img = to_tensor(img)\n",
    "img.unsqueeze_(0)  # 添加批次维度\n",
    "\n",
    "# 使用ONNX模型对图像进行超分辨率处理\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "img_out = ort_outs[0]\n",
    "\n",
    "# 输出图像在 ONNX 的输出\n",
    "print(\"输出图像形状:\", img_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26372bb1",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本notebook成功演示了PyTorch模型到ONNX的完整转换流程：\n",
    "\n",
    "### 主要步骤\n",
    "1. **模型准备**: 加载并设置PyTorch模型为评估模式\n",
    "2. **ONNX导出**: 使用`torch.onnx.export()`将模型导出为ONNX格式\n",
    "3. **模型验证**: 使用ONNX库验证导出模型的正确性\n",
    "4. **推理测试**: 使用ONNX Runtime进行推理并与原模型对比\n",
    "5. **性能评估**: 比较两种推理方式的执行时间\n",
    "6. **实际应用**: 在真实图像上测试分类效果\n",
    "\n",
    "### 关键优势\n",
    "- **跨平台部署**: ONNX格式支持多种推理引擎和硬件平台\n",
    "- **性能优化**: ONNX Runtime通常提供更好的推理性能\n",
    "- **模型兼容性**: 保持与原始PyTorch模型相同的推理结果\n",
    "\n",
    "### 输出文件\n",
    "- `../temp_data/test.onnx`: 导出的ONNX模型文件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
